<!DOCTYPE html>
<!-- saved from url=(0082)https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python -->
<html lang="en" data-theme="day"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  
  

<meta name="citation_title" content="Introduction to stylometry with Python
">

<meta name="citation_author" content="François Dominic Laramée">

<meta name="citation_publication_date" content="2018-04-21">
<meta name="citation_journal_title" content="Programming Historian">
<meta name="citation_public_url" content="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python">

  

  
  <!--https://dev.twitter.com/cards/-->







<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@ProgHist">
<meta name="twitter:title" content="Introduction to stylometry with Python
">


    <meta name="twitter:description" content="In this lesson you will learn to conduct &#39;stylometric analysis&#39; on texts and determine authorship of disputed texts. The lesson covers three methods: Mendenhall&#39;s Characteristic Curves of Compos...">


<meta name="twitter:image" content="https://programminghistorian.org/gallery/introduction-to-stylometry-with-python.png">

  

  <link rel="shortcut icon" href="https://programminghistorian.org/images/favicons/en_favicon.ico" type="image/x-icon">

  <!-- Mobile viewport optimized: h5bp.com/viewport -->
  <meta name="viewport" content="width=device-width">

  <link rel="stylesheet" href="./Introduction to stylometry with Python _ Programming Historian_files/all.css" integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9" crossorigin="anonymous">

  <link href="https://programminghistorian.org/feed.xml" rel="alternate" type="application/atom+xml">

  <title>Introduction to stylometry with Python
 | Programming Historian</title>

  <link href="./Introduction to stylometry with Python _ Programming Historian_files/css" rel="stylesheet">

  <link rel="stylesheet" href="./Introduction to stylometry with Python _ Programming Historian_files/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous" media="all">
  <link rel="stylesheet" href="./Introduction to stylometry with Python _ Programming Historian_files/github.css">
  <link rel="stylesheet" href="./Introduction to stylometry with Python _ Programming Historian_files/style.css">

  <script type="text/javascript" async="" src="./Introduction to stylometry with Python _ Programming Historian_files/ga.js"></script><script src="./Introduction to stylometry with Python _ Programming Historian_files/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="./Introduction to stylometry with Python _ Programming Historian_files/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
  <script src="./Introduction to stylometry with Python _ Programming Historian_files/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  <script type="text/javascript" src="./Introduction to stylometry with Python _ Programming Historian_files/ext_links.js"></script>
  <script type="text/javascript" src="./Introduction to stylometry with Python _ Programming Historian_files/header_links.js"></script>

  
  

  <script src="./Introduction to stylometry with Python _ Programming Historian_files/bootstrap-4-navbar.js"></script>

</head>

  <body>
    <main>
      <div class="hide-screen">
        
        

<div class="alert alert-success sitewide-alert text-center">
  <h2><a href="https://www.patreon.com/theprogramminghistorian" class="alert-link" target="_blank">Donate to <i>The Programming Historian</i> today!</a></h2>

</div>

        
        













<nav class="hide-screen navbar navbar-toggleable-md navbar-dark bg-dark" style="background-color: #444444" role="navigation">
  <!--<div class="container">-->
  <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <a class="navbar-brand" href="https://programminghistorian.org/">The Programming
    Historian</a>
  <div class="collapse navbar-collapse" id="navbarNavDropdown">
    <ul class="nav navbar-nav ml-auto w-100 justify-content-end" role="menubar">
      <li class="nav-item dropdown mobile-drop" role="menu">
        <a class="nav-link dropdown-toggle" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false" role="button" target="_blank">
          About
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
          <a class="dropdown-item" href="https://programminghistorian.org/en/about" role="menuitem">About PH</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/project-team" role="menuitem">Project Team</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/research" role="menuitem">Research</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/privacy-policy" role="menuitem">Privacy Policy</a>
        </div>
      </li>

      <li class="nav-item dropdown mobile-drop" role="menu">
        <a class="nav-link dropdown-toggle" id="navbarDropdownMenuLink2" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false" role="button" target="_blank">
          Contribute
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink2">
          <a class="dropdown-item" href="https://programminghistorian.org/en/contribute" role="menuitem">Overview</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/feedback" role="menuitem">Report a bug</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/reviewer-guidelines" role="menuitem">Reviewer Guidelines</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/author-guidelines" role="menuitem">Author Guidelines</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/translator-guidelines" role="menuitem">Translator Guidelines</a>
          <a class="dropdown-item" href="https://programminghistorian.org/en/editor-guidelines" role="menuitem">Editor Guidelines</a>
          <a class="dropdown-item" href="https://github.com/programminghistorian/jekyll/wiki/Making-Technical-Contributions" role="menuitem" target="_blank">Technical Contributions</a>
        </div>
      </li>

      <li class="nav-item" role="menuitem">
        <a class="nav-link" href="https://programminghistorian.org/en/lessons" role="link">Lessons</a>
      </li>

      <li class="nav-item dropdown mobile-drop" role="menu">
        <a class="nav-link dropdown-toggle" id="navbarDropdownMenuLink3" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false" role="button" target="_blank">
          Support Us
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink3">
        <a class="dropdown-item" href="https://programminghistorian.org/en/ipp" role="menuitem">Institutional Partnership Programme</a>
        <a class="dropdown-item" href="https://programminghistorian.org/en/individual" role="menuitem">Individual Supporters</a>
        <a class="dropdown-item" href="https://programminghistorian.org/en/supporters" role="menuitem">Our Supporters</a>
      </div></li>
      
      <li class="nav-item" role="menuitem">
        <a class="nav-link" href="https://programminghistorian.org/blog" role="link">Blog</a>
      </li>
     

      
      

      <li class="nav-item" role="menu">
        <div class="btn-group" role="menuitem" aria-label="Language selector">
          

          
          <a class="btn btn-secondary nav-link active disabled" role="button" aria-pressed="true" target="_blank">en</a>
          
          

          

          

          
          <a class="btn btn-secondary nav-link" role="button" href="https://programminghistorian.org/es">es</a>
          
          
          

          

          

          
          <a class="btn btn-secondary nav-link" role="button" href="https://programminghistorian.org/fr/lecons/introduction-a-la-stylometrie-avec-python">fr</a>
          
          
          

          

          

          
          <a class="btn btn-secondary nav-link" role="button" href="https://programminghistorian.org/pt">pt</a>
          
          
          
        </div>
      </li>
      <li class="nav-item" id="low-contrast-button" role="menu">
        <div class="theme-switch-wrapper">
          <label class="theme-switch" role="menuitem">
            <input type="checkbox" role="menuitemcheckbox">
            <span class="slider round">
              <span class="visually-hidden">night mode toggle</span>
            </span>
          </label>
        </div>
      </li>
    </ul>
  </div>
  <!--</div>-->
</nav>

<nav class="hide-print print-header navbar-brand">The Programming Historian </nav>

<script>
  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  function setTheme(currentTheme) {
    document.documentElement.setAttribute('data-theme', currentTheme);
    sessionStorage.setItem('theme', currentTheme);
  }
  
  let theme = sessionStorage.getItem('theme') === null ? 'day' : sessionStorage.getItem('theme');

  toggleSwitch.checked = theme === 'night' ? true : false;

  setTheme(theme);
  
  toggleSwitch.addEventListener('change', function(e){
    setTheme(e.target.checked ? 'night': 'day')
  }, false);

</script>
      </div>
      <div class="hide-print print-header">
        <h1>The Programming Historian</h1>
      </div>
      


















<header>

  <div class="container-fluid">

    <div class="container expanded">

      <div class="row">
        <div class="col-md-4">
          <div class="header-image rounded">
            <img src="./Introduction to stylometry with Python _ Programming Historian_files/introduction-to-stylometry-with-python.png" alt="A woman reading next to a painting">
          </div>
        </div>

        <div class="col-md-8">
          <div class="header-title">
            <h1><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python">Introduction to stylometry with Python
</a></h1>
          </div>

          <div class="header-author">
            <h2>



François Dominic Laramée <a href="https://orcid.org/0000-0001-5542-3754" target="_blank"><img src="./Introduction to stylometry with Python _ Programming Historian_files/ORCIDiD_iconvector.svg" alt="ORCID id icon" width="16px" style="max-width:16px;display:inline;"></a> </h2>
          </div>

          <div class="header-abstract">
            <p>In this lesson you will learn to conduct ‘stylometric analysis’ on texts and determine authorship of disputed texts. The lesson covers three methods: Mendenhall’s Characteristic Curves of Composition, Kilgariff’s Chi-Squared Method, and John Burrows’ Delta Method.</p>

          </div>

          <div class="container expanded">
            <div class="row d-flex justify-content-left">
              
              <div class="peer-review mr-5">
                <p>
                  <a href="https://github.com/programminghistorian/ph-submissions/issues/147" target="_blank">
                    <i class="fas fa-user-check"></i> Peer-reviewed
                    </a>
                </p>
              </div>
              
              <div class="open-license mr-5">
                <p><a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank"><i class="fas fa-lock-open"></i> CC-BY
                    4.0</a></p>
              </div>
              <div class="donate mr-5">
                <p><a href="https://programminghistorian.org/en/individual"><i class="fas fa-credit-card"></i> Support PH</a></p>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
  

  <div class="container-fluid header-helpers">
    <div class="container expanded">
      
      
      <div class="col-6 p-0 m-0">
        
          <div class="d-flex flex-wrap flex-md-row flex-column justify-content-between">
            <div>
              <h3>edited by</h3>
              <ul>
                
                
                <li>Adam Crymble</li>
                
              </ul>
            </div>

            <div>
              <h3>reviewed by</h3>
              <ul>
                
                
                <li>Folgert Karsdorp
                </li>
                
                
                <li>Jan Rybicki
                </li>
                
                
                <li>Antonio Rojas Castro <a href="https://orcid.org/0000-0002-8916-4997" target="_blank"><img src="./Introduction to stylometry with Python _ Programming Historian_files/ORCIDiD_iconvector.svg" alt="ORCID id icon" width="16px" style="max-width:16px;display:inline;"></a>
                </li>
                
              </ul>
            </div>

            

            

            

            
          </div>
        </div>
      </div> <!-- end row -->
    </div>

    <div class="container-fluid header-bottom">
      <div class="container expanded">
        <div class="d-flex flex-wrap flex-md-row flex-column justify-content-between">
          <div class="metarow">
            <h4>published</h4> | 2018-04-21
          </div>
          
          
          <div class="metarow">
            <h4>modified</h4> | <span id="modified-date">2020-10-21</span>
          </div>
          
          
          <div class="metarow">
            <h4>difficulty</h4> | 

Medium

          </div>
          
          <div class="metarow">
           <p> <img src="./Introduction to stylometry with Python _ Programming Historian_files/doi_icon.jpg" alt="DOI id icon" width="16px" style="max-width:16px;display:inline;"> https://doi.org/10.46430/phen0078</p>
          
          </div>
        </div>
      </div>
    </div>

</header>

<div class="container">

  
  
<div class="alert alert-success hide-screen"><h2 id="donate-today">Donate today!<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#donate-today"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">donate-today</span></a></h2>
<p>Great Open Access tutorials cost money to produce. Join the growing number of people <a href="https://www.patreon.com/theprogramminghistorian" target="_blank">supporting <em>The Programming Historian</em></a> so we can continue to share knowledge free of charge.</p>
</div>

  

  

  <div class="alert alert-warning">
    <!-- Banner pointing to the original and other translations of this lesson when they exist -->
    Available in:

    <a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python"> EN
    </a> (original) |

    

    <a href="https://programminghistorian.org/fr/lecons/introduction-a-la-stylometrie-avec-python"> FR </a>
    

    
  </div>
  

  <!-- Check if lesson is part of a sequence -->
  


  

  




  <div class="content">
    
<h2 class="no_toc" id="contents">Contents<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#contents"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">contents</span></a></h2>

<ul id="markdown-toc">
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#learning-outcomes" id="markdown-toc-learning-outcomes">Learning Outcomes</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#prior-reading" id="markdown-toc-prior-reading">Prior Reading</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#required-materials" id="markdown-toc-required-materials">Required materials</a>        <ul>
          <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-dataset" id="markdown-toc-the-dataset">The Dataset</a></li>
          <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-software" id="markdown-toc-the-software">The Software</a></li>
        </ul>
      </li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#some-notes-about-language-independence" id="markdown-toc-some-notes-about-language-independence">Some Notes about Language Independence</a></li>
    </ul>
  </li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-federalist-papers---historical-context" id="markdown-toc-the-federalist-papers---historical-context">The <em>Federalist Papers</em> - Historical Context</a></li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#our-test-cases" id="markdown-toc-our-test-cases">Our Test Cases</a></li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#preparing-the-data-for-analysis" id="markdown-toc-preparing-the-data-for-analysis">Preparing the Data for Analysis</a></li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#first-stylometric-test-mendenhalls-characteristic-curves-of-composition" id="markdown-toc-first-stylometric-test-mendenhalls-characteristic-curves-of-composition">First Stylometric Test: Mendenhall’s Characteristic Curves of Composition</a></li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#second-stylometric-test-kilgariffs-chi-squared-method" id="markdown-toc-second-stylometric-test-kilgariffs-chi-squared-method">Second Stylometric Test: Kilgariff’s Chi-Squared Method</a>    <ul>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#a-note-about-parts-of-speech" id="markdown-toc-a-note-about-parts-of-speech">A Note about Parts of Speech</a></li>
    </ul>
  </li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#third-stylometric-test-john-burrows-delta-method-advanced" id="markdown-toc-third-stylometric-test-john-burrows-delta-method-advanced">Third Stylometric Test: John Burrows’ Delta Method (Advanced)</a>    <ul>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#our-test-case" id="markdown-toc-our-test-case">Our Test Case</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#feature-selection" id="markdown-toc-feature-selection">Feature Selection</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-features-for-each-subcorpus" id="markdown-toc-calculating-features-for-each-subcorpus">Calculating features for each subcorpus</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-feature-averages-and-standard-deviations" id="markdown-toc-calculating-feature-averages-and-standard-deviations">Calculating feature averages and standard deviations</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-z-scores" id="markdown-toc-calculating-z-scores">Calculating z-scores</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-features-and-z-scores-for-our-test-case" id="markdown-toc-calculating-features-and-z-scores-for-our-test-case">Calculating features and z-scores for our test case</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-delta" id="markdown-toc-calculating-delta">Calculating Delta</a></li>
    </ul>
  </li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#further-reading-and-resources" id="markdown-toc-further-reading-and-resources">Further Reading and Resources</a>    <ul>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#interesting-case-studies" id="markdown-toc-interesting-case-studies">Interesting case studies</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#additional-references-on-authorship-and-stylometry" id="markdown-toc-additional-references-on-authorship-and-stylometry">Additional references on authorship and stylometry</a></li>
      <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#varia" id="markdown-toc-varia">Varia</a></li>
    </ul>
  </li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#acknowledgements" id="markdown-toc-acknowledgements">Acknowledgements</a></li>
  <li><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#endnotes" id="markdown-toc-endnotes">Endnotes</a></li>
</ul>

<h1 id="introduction">Introduction</h1>

<p><a href="https://en.wikipedia.org/wiki/Stylometry" target="_blank">Stylometry</a> is the quantitative study of literary style through computational <a href="https://en.wikipedia.org/wiki/Close_reading" target="_blank">distant reading</a> methods. It is based on the observation that authors tend to write in relatively consistent, recognizable and unique ways. For example:</p>

<ul>
  <li>Each person has their own unique vocabulary, sometimes rich, sometimes limited. Although a larger vocabulary is usually associated with literary quality, this is not always the case. Ernest Hemingway is famous for using a surprisingly small number of different words in his writing,<sup id="fnref:1" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:1" class="footnote" rel="footnote">1</a></sup> which did not prevent him from winning the Nobel Prize for Literature in 1954.</li>
  <li>Some people write in short sentences, while others prefer long blocks of text consisting of many clauses.</li>
  <li>No two people use semicolons, em-dashes, and other forms of punctuation in the exact same way.</li>
</ul>

<p>The ways in which writers use small <a href="https://en.wikipedia.org/wiki/Function_word" target="_blank">function words</a>, such as articles, prepositions and conjunctions, has proven particularly telling. In a survey of historical and current stylometric methods, Efstathios Stamatatos points out that function words are “used in a largely unconscious manner by the authors, and they are topic-independent.”<sup id="fnref:2" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:2" class="footnote" rel="footnote">2</a></sup> For stylometric analysis, this is very advantageous, as such an unconscious pattern is likely to vary less, over an author’s <a href="https://en.wikipedia.org/wiki/Text_corpus" target="_blank">corpus</a>, than his or her general vocabulary. (It is also very hard for a would-be forger to copy.) Function words have also been identified as important markers of literary genre and of chronology.</p>

<p>Scholars have used stylometry as a tool to study a variety of cultural questions. For example, a considerable amount of research has studied the differences between the ways in which men and women write<sup id="fnref:3" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:3" class="footnote" rel="footnote">3</a></sup> or are written about.<sup id="fnref:4" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:4" class="footnote" rel="footnote">4</a></sup> Other scholars have studied the ways in which a sudden change in writing style within a single text may indicate plagiarism,<sup id="fnref:5" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:5" class="footnote" rel="footnote">5</a></sup> and even the manner in which lyrics penned by musicians John Lennon and Paul McCartney grew progressively less cheerful and less active as the <a href="https://en.wikipedia.org/wiki/The_Beatles" target="_blank">Beatles</a> approached the end of their recording career in the 1960s.<sup id="fnref:6" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:6" class="footnote" rel="footnote">6</a></sup></p>

<p>However, one of the most common applications of stylometry is in <a href="https://en.wikipedia.org/w/index.php?title=Authorship_attribution&amp;redirect=no" target="_blank">authorship attribution</a>. Given an anonymous text, it is sometimes possible to guess who wrote it by measuring certain features, like the average number of words per sentence or the propensity of the author to use “while” instead of “whilst”, and comparing the measurements with other texts written by the suspected author. This is what we will be doing in this lesson, using as our test case perhaps the most famous instance of disputed authorship in political writing history, that of the <em>Federalist Papers.</em></p>

<h2 id="learning-outcomes">Learning Outcomes<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#learning-outcomes"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">learning-outcomes</span></a></h2>

<p>At the end of this lesson, we will have examined the following topics:</p>

<ul>
  <li>How to apply several stylometric methods to infer authorship of an anonymous text or set of texts.</li>
  <li>How to use relatively advanced data structures, including <a href="https://en.wikipedia.org/wiki/Data_dictionary" target="_blank">dictionaries</a> of <a href="https://en.wikipedia.org/wiki/String_(computer_science)" target="_blank">strings</a> and dictionaries of dictionaries, in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)" target="_blank">Python</a>.</li>
  <li>The basics of the <a href="https://www.nltk.org/" target="_blank">Natural Languate Toolkit</a> (NLTK), a popular Python module dedicated to <a href="https://en.wikipedia.org/wiki/Natural-language_processing" target="_blank">natural language processing</a>.</li>
</ul>

<div class="alert alert-warning">
Please note that the code in this lesson has been designed to run sequentially. Should you want to bypass Mendenhall's method, for example, and move straight to Kilgariff's or Burrows', please make sure to copy-paste the preprocessing code found in the description of Menenhall's characteristic curves into your own code block. Otherwise, you will not be able to match the results presented here. 
</div>

<h2 id="prior-reading">Prior Reading<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#prior-reading"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">prior-reading</span></a></h2>

<p>If you do not have experience with the Python programming language or are finding examples in this tutorial difficult, the author recommends you read the lessons on <a href="https://programminghistorian.org/lessons/working-with-text-files">Working with Text Files in Python</a> and <a href="https://programminghistorian.org/lessons/manipulating-strings-in-python">Manipulating Strings in Python</a>. Please note, that those lessons were written in Python version 2 whereas this one uses Python version 3. The differences in <a href="https://en.wikipedia.org/wiki/Syntax" target="_blank">syntax</a> between the two versions of the language can be subtle. If you are confused at any time, follow the examples as written in this lesson and use the other lessons as background material. (More precisely, the code in this tutorial was written using <a href="https://www.python.org/downloads/release/python-364/" target="_blank">Python 3.6.4</a>; the <a href="https://docs.python.org/3/whatsnew/3.6.html#pep-498-formatted-string-literals" target="_blank">f-string construct</a> in the line <code class="language-plaintext highlighter-rouge">with open(f'data/federalist_{filename}.txt') as f:</code>, for example, requires Python 3.6 or a more recent version of the language.)</p>

<h2 id="required-materials">Required materials<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#required-materials"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">required-materials</span></a></h2>

<p>This tutorial uses both datasets and software that you will have to download and install.</p>

<h3 id="the-dataset">The Dataset<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-dataset"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">the-dataset</span></a></h3>

<p>To work through this lesson, you will need to download and unzip the archive of the <em>Federalist Papers</em> (<a href="https://github.com/programminghistorian/ph-submissions/tree/gh-pages/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip" target="_blank">.zip</a>) containing the 85 documents that we will use for our analysis. The archive also contains the <a href="https://www.gutenberg.org/cache/epub/1404/pg1404.txt" target="_blank">original Project Gutenberg ebook</a> version of the <em>Federalist Papers</em> from which these 85 documents have been extracted. When you unzip the archive, it will create a <a href="https://en.wikipedia.org/wiki/Directory_(computing)" target="_blank">directory</a> called <code class="language-plaintext highlighter-rouge">data</code> in your current <a href="https://en.wikipedia.org/wiki/Working_directory" target="_blank">working directory</a>. Make sure that you stay in this current working directory and that you save all work here while completing the lesson.</p>

<h3 id="the-software">The Software<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-software"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">the-software</span></a></h3>

<p>This lesson uses the following Python language versions and <a href="https://en.wikipedia.org/wiki/Library_(computing)" target="_blank">libraries</a>:</p>

<ul>
  <li><a href="https://www.python.org/downloads/" target="_blank">Python 3.x</a> - the latest stable version is recommended.</li>
  <li><a href="https://www.nltk.org/" target="_blank">nltk</a> - Natural Language Toolkit, usually abbreviated <code class="language-plaintext highlighter-rouge">nltk</code>.</li>
  <li><a href="https://matplotlib.org/" target="_blank">matplotlib</a></li>
</ul>

<p>Some of these modules may not be pre-installed on your computer. Should you encounter error messages such as: “Module not found” or similar, you will have to download and install the missing module(s). This is easiest to accomplish using the <code class="language-plaintext highlighter-rouge">pip</code> command. Full details are available via the <em>Programming Historian</em> lesson on <a href="https://programminghistorian.org/lessons/installing-python-modules-pip">Installing Python modules with pip</a>.</p>

<h2 id="some-notes-about-language-independence">Some Notes about Language Independence<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#some-notes-about-language-independence"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">some-notes-about-language-independence</span></a></h2>

<p>This tutorial applies stylometric analysis to a set of English-language texts using a Python library called <code class="language-plaintext highlighter-rouge">nltk</code>. Much of the functionality provided by the <code class="language-plaintext highlighter-rouge">nltk</code> works with other languages. As long as a language provides a clear way to distinguish word boundaries within a word, <code class="language-plaintext highlighter-rouge">nltk</code> should perform well. Languages such as Chinese for which there is no clear distinction between word boundaries may be problematic. I have used <code class="language-plaintext highlighter-rouge">nltk</code> with French texts without any trouble; other languages that use <a href="https://en.wikipedia.org/wiki/Diacritic" target="_blank">diacritics</a>, such as Spanish and German, should also work well with <code class="language-plaintext highlighter-rouge">nltk</code>.  Please refer to <a href="http://www.nltk.org/book/" target="_blank">nltk’s documentation</a> for details.</p>

<p>Only one of the tasks in this tutorial requires language-dependent code. To divide a text into a set of French or Spanish words, you will need to specify the appropriate language as a parameter to <code class="language-plaintext highlighter-rouge">nltk</code>’s <a href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" target="_blank">tokenizer</a>, which uses English as the default. This will be explained in the tutorial.</p>

<p>Finally, note that some linguistic tasks, such as <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" target="_blank">part-of-speech tagging</a>, may not be supported by <code class="language-plaintext highlighter-rouge">nltk</code> in languages other than English. This tutorial does not cover part-of-speech tagging. Should you need it for your own projects, please refer to the <a href="http://www.nltk.org/book/" target="_blank">nltk documentation</a> for advice.</p>

<h1 id="the-federalist-papers---historical-context">The <em>Federalist Papers</em> - Historical Context</h1>

<p>The <em><a href="https://en.wikipedia.org/wiki/The_Federalist_Papers" target="_blank">Federalist Papers</a></em> (also known simply as the <em>Federalist</em>) are a collection of 85 seminal political theory articles published between October 1787 and May 1788. These papers, written as the debate over the ratification of the Constitution of the United States was raging, presented the case for the system of government that the U.S. ultimately adopted and under which it lives to this day. As such, the <em>Federalist</em> is sometimes described as America’s greatest and most lasting contribution to the field of political philosophy.</p>

<p>Three of the Early Republic’s most prominent men wrote the papers:</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Alexander_Hamilton" target="_blank">Alexander Hamilton</a>, first Secretary of the Treasury of the United States.</li>
  <li><a href="https://en.wikipedia.org/wiki/James_Madison" target="_blank">James Madison</a>, fourth President of the United States and the man sometimes called the “Father of the Constitution” for his key role at the 1787 Constitutional Convention.</li>
  <li><a href="https://en.wikipedia.org/wiki/John_Jay" target="_blank">John Jay</a>, first Chief Justice of the United States, second governor of the State of New York, and diplomat.</li>
</ul>

<p>However, <em>who</em> wrote <em>which</em> of the papers was a matter of open debate for 150 years, and the co-authors’ behavior is to blame for the mystery.</p>

<p>First, the <em>Federalist</em> was published anonymously under the shared pseudonym “Publius”. Anonymous publication was not uncommon in the eighteenth century, especially in the case of politically sensitive material. However, in the <em>Federalist</em>’s case, the fact that three people shared a single pseudonym makes it difficult to determine who wrote which part of the text. Compounding the problem is the fact that the three authors wrote about closely related topics, at the same time, and using the same cultural and political references, which made their respective vocabularies hard to distinguish from each other.</p>

<p>Second, because Madison and Hamilton left conflicting testimonies regarding their roles in the project. In a famous 1944 article, historian Douglass Adair<sup id="fnref:7" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:7" class="footnote" rel="footnote">7</a></sup> explained that neither man wanted the true authorship of the Papers to become public knowledge during their lifetimes, because they had come to regret some of what they had written. The notoriously vainglorious Hamilton, however, wanted to make sure that <em>posterity</em> would remember him as the driving force behind the Papers. In 1804, two days before he was to fight a duel (in which he was killed), Hamilton wrote a note claiming 63 of the 85 Papers as his own work and gave it to a friend for safekeeping. Ten years later, Madison refuted some of Hamilton’s claims, stating that <em>he</em> was the author of 12 of the papers on Hamilton’s list and that he had done most of the work on three more for which Hamilton claimed equal credit. Since Hamilton was long dead, it was impossible for him to respond to Madison.</p>

<p>Third, because in the words of David Holmes and Richard Forsyth,<sup id="fnref:8" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:8" class="footnote" rel="footnote">8</a></sup> Madison and Hamilton had “unusually similar” writing styles. Frederick Mosteller and Frederick Williams calculated that, in the papers for which authorship is not in doubt, the average lengths of the sentences written by the two men are both uncommonly high and virtually identical: 34.59 and 34.55 words respectively.<sup id="fnref:9" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:9" class="footnote" rel="footnote">9</a></sup> The <a href="https://en.wikipedia.org/wiki/Standard_deviation" target="_blank">standard deviations</a> in the lengths of the two men’s sentences are also nearly identical. And as Mosteller quipped, neither man was known to use a short word when a long one would do. Thus, there was no easy way to pinpoint any given paper as clearly marked with Hamilton’s or Madison’s stylistic signature.</p>

<p>It wasn’t until 1964 that Mosteller and David Lee Wallace<sup id="fnref:10" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:10" class="footnote" rel="footnote">10</a></sup>, using word usage statistics, came up with a relatively satisfactory solution to the mystery. By comparing how often Madison and Hamilton used common words like <em>may</em>, <em>also</em>, <em>an</em>, <em>his</em>, etc., they concluded that the disputed papers had all been written by Madison. Even in the case of <em>Federalist 55</em>, the paper for which they said that the evidece was the least convincing, Mosteller and Wallace estimated the odds that Madison was the author at 100 to 1.</p>

<p>Since then, the authorship of the <em>Federalist</em> has remained a common test case for <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank">machine learning</a> algorithms in the English-speaking world.<sup id="fnref:11" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:11" class="footnote" rel="footnote">11</a></sup> Stylometric analysis has also continued to use the <em>Federalist</em> to refine its methods, for example as a test case while looking for signs of hidden collaborations between multiple authors in a single text.<sup id="fnref:12" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:12" class="footnote" rel="footnote">12</a></sup> Interestingly, some of the results of this research suggest that the answer to the <em>Federalist</em> mystery may not be quite as clear-cut as Mosteller and Wallace thought, and that Hamilton and Madison may have co-written more of the <em>Federalist</em> than we ever suspected.</p>

<h1 id="our-test-cases">Our Test Cases</h1>

<p>In this lesson, we will use the <em>Federalist</em> as a case study to demonstrate three different stylometric approaches.</p>

<ol>
  <li>Mendenhall’s Characteristic Curves of Composition</li>
  <li>Kilgariff’s Chi-Squared Method</li>
  <li>John Burrows’ Delta Method</li>
</ol>

<p>This will require splitting the papers into six categories:</p>

<ol>
  <li>The 51 papers known to have been written by Alexander Hamilton.</li>
  <li>The 14 papers known to have been written by James Madison.</li>
  <li>Four of the five papers known to have been written by John Jay.</li>
  <li>Three papers that were probably co-written by Madison and Hamilton and for which Madison claimed principal authorship.</li>
  <li>The 12 papers disputed between Hamilton and Madison.</li>
  <li><em>Federalist 64</em> in a category of its own.</li>
</ol>

<p>This division mostly follows Mosteller’s lead<sup id="fnref:13" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:13" class="footnote" rel="footnote">13</a></sup>. The one exception is <em>Federalist 64</em>, which everyone agrees was written by John Jay but which we keep in a separate category for reasons that will become clear later.</p>

<p>Our first two tests, using T. C. Mendenhall’s characteristic curves of composition and Adam Kilgariff’s <a href="https://en.wikipedia.org/wiki/Chi-squared_test" target="_blank">chi-squared</a> distance, will look at the 12 disputed papers as a group, to see whether they resemble anyone’s writing in particular. Then, in our third and final test, we will apply John Burrows’ Delta method to look at <em>Federalist 64</em> and to confirm whether it was, indeed, written by John Jay.</p>

<h1 id="preparing-the-data-for-analysis">Preparing the Data for Analysis</h1>

<p>Before we can proceed with stylometric analysis, we need to load the files containing all 85 papers into convenient <a href="https://en.wikipedia.org/wiki/Data_structure" target="_blank">data structures</a> in computer memory.</p>

<p>The first step in this process is to assign each of the 85 papers to the proper set. Since we have given each paper standardized names from <code class="language-plaintext highlighter-rouge">federalist_1.txt</code> to <code class="language-plaintext highlighter-rouge">federalist_85.txt</code>, it is possible to assign each paper to its author (or to its test set, if we want to learn its author’s identity) using a Python <em>dictionary</em>. The dictionary is a data type made up of an arbitrary number of key-value pairs; in this case, the names of authors will serve as keys, while the lists of paper numbers will be the values associated with these keys.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">papers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'Madison'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">],</span>
    <span class="s">'Hamilton'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span>
                 <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span>
                 <span class="mi">61</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">68</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">73</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">76</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span>
                 <span class="mi">78</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">83</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">85</span><span class="p">],</span>
    <span class="s">'Jay'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="s">'Shared'</span><span class="p">:</span> <span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s">'Disputed'</span><span class="p">:</span> <span class="p">[</span><span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">63</span><span class="p">],</span>
    <span class="s">'TestCase'</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Python dictionaries are very flexible. For example, we can access a value by <em>indexing</em> the dictionary with one of its keys, we can scan the entire dictionary by looping over its list of keys, etc. We will make ample use of this functionality as we move along.</p>

<p>Next, as we are interested in each author’s vocabulary, we will define a short Python <a href="https://en.wikipedia.org/wiki/Subroutine" target="_blank">function</a> that creates a long listing of the words in each of the papers assigned to a single author. This will be stored as a <a href="https://en.wikipedia.org/wiki/String_(computer_science)" target="_blank">string</a>. Open your chosen Python development environment. If you do not know how to do this, you should read Setting up an Integrated Development Environment (<a href="https://programminghistorian.org/lessons/mac-installation">Mac</a>), (<a href="https://programminghistorian.org/lessons/linux-installation">Linux</a>), (<a href="https://programminghistorian.org/lessons/windows-installation">Windows</a>) before continuing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A function that compiles all of the text files associated with a single author into a single string
</span><span class="k">def</span> <span class="nf">read_files_into_string</span><span class="p">(</span><span class="n">filenames</span><span class="p">):</span>
    <span class="n">strings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s">'data/federalist_</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">.txt'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">strings</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
    <span class="k">return</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">strings</span><span class="p">)</span>
</code></pre></div></div>

<p>Third, we build a new data structure by repeatedly calling the <code class="language-plaintext highlighter-rouge">read_files_into_string()</code> function, passing it a different list of papers every time. We will store the results into another dictionary, this one with author/test case names as keys and all of the text of the relevant papers as values. For simplicity’s sake, we will refer to the string containing a list of papers as “the author’s corpus”, even when we are dealing with disputed or shared papers rather than with an individual’s known contribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make a dictionary out of the authors' corpora
</span><span class="n">federalist_by_author</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">author</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">papers</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">federalist_by_author</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_files_into_string</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>
</code></pre></div></div>

<p>To make sure that the files loaded properly, print the first hundred characters of each dictionary entry to screen:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">papers</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">federalist_by_author</span><span class="p">[</span><span class="n">author</span><span class="p">][:</span><span class="mi">100</span><span class="p">])</span>
</code></pre></div></div>

<p>If this printing operation yields anything at all, then the file input operation has worked as expected and you can move on to stylometric analysis.</p>

<div class="alert alert-warning">
If the files fail to load, the most likely reason is that your current working directory is not the `data` repository created by unzipping the archive from the Required Materials section above; changing your working directory should do the trick. How you do this depends on your Python development environment.
</div>

<h1 id="first-stylometric-test-mendenhalls-characteristic-curves-of-composition">First Stylometric Test: Mendenhall’s Characteristic Curves of Composition</h1>

<p>Literary scholar T. C. Mendenhall once wrote that an author’s stylistic signature could be found by counting how often he or she used words of different lengths.<sup id="fnref:14" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:14" class="footnote" rel="footnote">14</a></sup> For example, if we counted word lengths in several 1,000-word or 5,000 word segments of any novel, and then plotted a graph of the word length distributions, the curves would look pretty much the same no matter what parts of the novel we had picked. Indeed, Mendenhall thought that if one counted enough words selected from various parts of a writer’s entire life’s work (say, 100,000 or so), the author’s “characteristic curve” of word length usage would become so precise that it would be constant over his or her lifetime.</p>

<p>By today’s standards, counting word lengths seems like a very blunt way of measuring literary style. Mendenhall’s method does not take the actual words in an author’s vocabulary into account, which is obviously problematic. Therefore, we should not treat the characteristic curves as a particularly trustworthy source of stylometric evidence. However, Mendenhall published his theory over one hundred and thirty years ago and made all calculations by hand. It is understandable that he would have chosen to work with a statistic that, however coarse, was at least easy to compile. In honor of the historical value of his early attempt at stylometry, and because the characteristic curve yields interesting visual results that can be implemented quickly, we will use Mendenhall’s method as a first step in our exploration of authorship attribution techniques.</p>

<p>The code required to calculate characteristic curves for the <em>Federalist</em>’s authors is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load nltk
</span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Compare the disputed papers to those written by everyone,
# including the shared ones.
</span><span class="n">authors</span> <span class="o">=</span> <span class="p">(</span><span class="s">"Hamilton"</span><span class="p">,</span> <span class="s">"Madison"</span><span class="p">,</span> <span class="s">"Disputed"</span><span class="p">,</span> <span class="s">"Jay"</span><span class="p">,</span> <span class="s">"Shared"</span><span class="p">)</span>

<span class="c1"># Transform the authors' corpora into lists of word tokens
</span><span class="n">federalist_by_author_tokens</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">federalist_by_author_length_distributions</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">federalist_by_author</span><span class="p">[</span><span class="n">author</span><span class="p">])</span>

    <span class="c1"># Filter out punctuation
</span>    <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="p">([</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span>
                                            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">token</span><span class="p">)])</span>

    <span class="c1"># Get a distribution of token lengths
</span>    <span class="n">token_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]]</span>
    <span class="n">federalist_by_author_length_distributions</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="n">token_lengths</span><span class="p">)</span>
    <span class="n">federalist_by_author_length_distributions</span><span class="p">[</span><span class="n">author</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="n">author</span><span class="p">)</span>
</code></pre></div></div>

<p>The ‘%matplotlib inline’ declaration below ‘import nltk’ is required if your development environment is a <a href="https://jupyter.org/" target="_blank">Jupyter Notebook</a>, as it was for me while writing this tutorial; otherwise you may not see the graphs on your screen. If you work in <a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html" target="_blank">Jupyter Lab</a>, please replace this clause with ‘%matplotlib ipympl’.</p>

<p>The first line in the code snippet above loads the <em>Natural Language Toolkit module (nltk)</em>, which contains an enormous number of useful functions and resources for text processing. We will barely touch its basics in this lesson; if you decide to explore text analysis in Python further, I strongly recommend that you start with <a href="https://www.nltk.org/" target="_blank">nltk’s documentation</a>.</p>

<p>The next few lines set up data structures that will be filled by the block of code within the <code class="language-plaintext highlighter-rouge">for</code> loop. This loop makes the same calculations for all of our “authors”:</p>

<ul>
  <li>It invokes <code class="language-plaintext highlighter-rouge">nltk</code>’s <code class="language-plaintext highlighter-rouge">word_tokenize()</code> method to chop an author’s corpus into its component <em>tokens</em>, i.e., words, numbers, punctuation, etc.;</li>
  <li>It looks at this list of tokens and filters out non-words;</li>
  <li>It creates a list containing the lengths of every word token that remains;</li>
  <li>It creates a <em>frequency distribution</em> object from this list of word lengths, basically counting how many one-letter words, two-letter words, etc., there are in the author’s corpus.</li>
  <li>It plots a graph of the distribution of word lengths in the corpus, for all words up to length 15.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">nltk.word_tokenize()</code> uses English rules by default. If you want to tokenize texts in another language, you will need to change one line in the code above to feed the proper language to the tokenizer as a parameter. For example:
<code class="language-plaintext highlighter-rouge">tokens = nltk.word_tokenize(federalist_by_author[author], language='french')</code>. Read the <a href="https://www.nltk.org/" target="_blank">nltk’s documentation</a> for more details.</p>

<p>The results should look like this:</p>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-1.jpg" alt="Figure 1: Mendenhall&#39;s curve for Hamilton.">
<figcaption>
    <p>Figure 1: Mendenhall’s curve for Hamilton.</p>

</figcaption>
</figure>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-2.jpg" alt="Figure 2: Mendenhall&#39;s curve for Madison.">
<figcaption>
    <p>Figure 2: Mendenhall’s curve for Madison.</p>

</figcaption>
</figure>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-3.jpg" alt="Figure 3: Mendenhall&#39;s curve for the disputed papers.">
<figcaption>
    <p>Figure 3: Mendenhall’s curve for the disputed papers.</p>

</figcaption>
</figure>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-4.jpg" alt="Figure 4: Mendenhall&#39;s curve for Jay.">
<figcaption>
    <p>Figure 4: Mendenhall’s curve for Jay.</p>

</figcaption>
</figure>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-5.jpg" alt="Figure 5: Mendenhall&#39;s curve for the papers co-authored by Madison and Hamilton.">
<figcaption>
    <p>Figure 5: Mendenhall’s curve for the papers co-authored by Madison and Hamilton.</p>

</figcaption>
</figure>

<p>As you can see from the graphs, the characteristic curve associated with the disputed papers looks like a compromise between Madison’s and Hamilton’s. The leftmost part of the disputed papers’ graph, which accounts for the most frequent word lengths, looks a bit more similar to Madison’s; the tail end of the graph, like Hamilton’s. This is consistent with the historical observation that Madison and Hamilton had similar styles, but it does not help us much with our authorship attribution task. The best that we can say is that John Jay almost certainly did <em>not</em> write the disputed papers, because his curve looks nothing like the others; lengths 6 and 7 are even inverted in his part of the corpus, compared to everyone else’s.</p>

<p>If we had no additional information to work with, we would tend to conclude that the disputed papers are probably Madison’s work, albeit without much confidence. Fortunately, stylometric science has advanced a great deal since Mendenhall’s time.</p>

<h1 id="second-stylometric-test-kilgariffs-chi-squared-method">Second Stylometric Test: Kilgariff’s Chi-Squared Method</h1>

<p>In a 2001 paper, Adam Kilgarriff<sup id="fnref:15" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:15" class="footnote" rel="footnote">15</a></sup> recommends using the chi-squared statistic to determine authorship. Readers familiar with statistical methods may recall that chi-squared is sometimes used to test whether a set of observations (say, voters’ intentions as stated in a poll) follow a certain <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank">probability distribution</a> or pattern. This is not what we are after here. Rather, we will simply use the statistic to measure the “distance” between the vocabularies employed in two sets of texts. The more similar the vocabularies, the likelier it is that the same author wrote the texts in both sets. This assumes that a person’s vocabulary and word usage patterns are relatively constant.</p>

<p>Here is how to apply the statistic for authorship attribution:</p>

<ul>
  <li>Take the corpora associated with two authors.</li>
  <li>Merge them into a single, larger corpus.</li>
  <li>Count the tokens for each of the words that can be found in this larger corpus.</li>
  <li>Select the <a href="https://en.wikipedia.org/wiki/Sample_(statistics)" target="_blank"><code class="language-plaintext highlighter-rouge">n</code></a> most common words in the larger corpus.</li>
  <li>Calculate how many tokens of these <code class="language-plaintext highlighter-rouge">n</code> most common words we would have expected to find in each of the two original corpora if they had come from the same author. This simply means dividing the number of tokens that we have observed in the combined corpus into two values, based on the relative sizes of the two authors’ contributions to the common corpus.</li>
  <li>Calculate a chi-squared distance by summing, over the <code class="language-plaintext highlighter-rouge">n</code> most common words, the <em>squares of the differences between the actual numbers of tokens found in each author’s corpus and the expected numbers</em>, divided by the expected numbers. Figure 6 shows the equation for the chi-squared statistic, where C(i) represents the observed number of tokens for feature ‘i’, and E(i), the expected number for this feature.</li>
</ul>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-6.jpg" alt="Figure 6: Equation for the chi-squared statistic.">
<figcaption>
    <p>Figure 6: Equation for the chi-squared statistic.</p>

</figcaption>
</figure>

<p>The smaller the chi-squared value, the more similar the two corpora. Therefore, we will calculate a chi-squared for the difference between the Madison and Disputed corpora, and another for the difference between the Hamilton and Disputed corpora; the smaller value will indicate which of Madison and Hamilton is the most similar to Disputed.</p>

<p>Note: No matter which stylometric method we use, the choice of <code class="language-plaintext highlighter-rouge">n</code>, the number of words to take into consideration, is something of a dark art. In the literature surveyed by Stamatatos<sup id="fnref:2:1" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:2" class="footnote" rel="footnote">2</a></sup>, scholars have suggested between 100 and 1,000 of the most common words; one project even used every word that appeared in the corpus at least twice. As a guideline, the larger the corpus, the larger the number of words that can be used as features without running the risk of giving undue importance to a word that occurs only a handful of times. In this lesson, we will use a relatively large <code class="language-plaintext highlighter-rouge">n</code> for the chi-squared method and a smaller one for the next method. Changing the value of <code class="language-plaintext highlighter-rouge">n</code> will certainly change the numeric results a little; however, if a small modification of <code class="language-plaintext highlighter-rouge">n</code> causes a change in authorship attribution, this is a sign that the test you are performing is unable to provide meaningful evidence regarding your test case.</p>

<p>The following code snippet implements Kilgariff’s method, with the frequencies of the 500 most common words in the joint corpus being used in the calculation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Who are the authors we are analyzing?
</span><span class="n">authors</span> <span class="o">=</span> <span class="p">(</span><span class="s">"Hamilton"</span><span class="p">,</span> <span class="s">"Madison"</span><span class="p">)</span>

<span class="c1"># Lowercase the tokens so that the same word, capitalized or not,
# counts as one word
</span><span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]])</span>
<span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="s">"Disputed"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="s">"Disputed"</span><span class="p">]])</span>

<span class="c1"># Calculate chisquared for each of the two candidate authors
</span><span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>

    <span class="c1"># First, build a joint corpus and identify the 500 most frequent words in it
</span>    <span class="n">joint_corpus</span> <span class="o">=</span> <span class="p">(</span><span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">+</span>
                    <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="s">"Disputed"</span><span class="p">])</span>
    <span class="n">joint_freq_dist</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="n">joint_corpus</span><span class="p">)</span>
    <span class="n">most_common</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">joint_freq_dist</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>

    <span class="c1"># What proportion of the joint corpus is made up
</span>    <span class="c1"># of the candidate author's tokens?
</span>    <span class="n">author_share</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">])</span>
                    <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">joint_corpus</span><span class="p">))</span>

    <span class="c1"># Now, let's look at the 500 most common words in the candidate
</span>    <span class="c1"># author's corpus and compare the number of times they can be observed
</span>    <span class="c1"># to what would be expected if the author's papers
</span>    <span class="c1"># and the Disputed papers were both random samples from the same distribution.
</span>    <span class="n">chisquared</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span><span class="n">joint_count</span> <span class="ow">in</span> <span class="n">most_common</span><span class="p">:</span>

        <span class="c1"># How often do we really see this common word?
</span>        <span class="n">author_count</span> <span class="o">=</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">].</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">disputed_count</span> <span class="o">=</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="s">"Disputed"</span><span class="p">].</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

        <span class="c1"># How often should we see it?
</span>        <span class="n">expected_author_count</span> <span class="o">=</span> <span class="n">joint_count</span> <span class="o">*</span> <span class="n">author_share</span>
        <span class="n">expected_disputed_count</span> <span class="o">=</span> <span class="n">joint_count</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">author_share</span><span class="p">)</span>

        <span class="c1"># Add the word's contribution to the chi-squared statistic
</span>        <span class="n">chisquared</span> <span class="o">+=</span> <span class="p">((</span><span class="n">author_count</span><span class="o">-</span><span class="n">expected_author_count</span><span class="p">)</span> <span class="o">*</span>
                       <span class="p">(</span><span class="n">author_count</span><span class="o">-</span><span class="n">expected_author_count</span><span class="p">)</span> <span class="o">/</span>
                       <span class="n">expected_author_count</span><span class="p">)</span>

        <span class="n">chisquared</span> <span class="o">+=</span> <span class="p">((</span><span class="n">disputed_count</span><span class="o">-</span><span class="n">expected_disputed_count</span><span class="p">)</span> <span class="o">*</span>
                       <span class="p">(</span><span class="n">disputed_count</span><span class="o">-</span><span class="n">expected_disputed_count</span><span class="p">)</span>
                       <span class="o">/</span> <span class="n">expected_disputed_count</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"The Chi-squared statistic for candidate"</span><span class="p">,</span> <span class="n">author</span><span class="p">,</span> <span class="s">"is"</span><span class="p">,</span> <span class="n">chisquared</span><span class="p">)</span>
</code></pre></div></div>

<p>The output produced by the chi-squared method should look like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Chi-squared statistic for candidate Hamilton is 3434.6850314768426
The Chi-squared statistic for candidate Madison is 1907.5992915766838
</code></pre></div></div>

<div class="alert alert-warning">
In the snippet above, we convert everything to lowercase so that we won't count word tokens that begin with a capital letter because they appear at the beginning of a sentence and lowercased tokens of the same word as two different words. Sometimes this may cause a few errors, for example when a proper noun and a common noun are written the same way except for capitalization, but usually it increases accuracy.
</div>

<p>As we can see from the above results, the chi-squared distance between the Disputed and Hamilton corpora is considerably larger than the distance between the Madison and Disputed corpora. This is a strong sign that, if a single author is responsible for the 12 papers in the Disputed corpus, that author is Madison rather than Hamilton.</p>

<p>However, chi-squared is still a coarse method. For one thing, words that appear very frequently tend to carry a disproportionate amount of weight in the final calculation. Sometimes this is fine; other times, subtle differences in style represented by the ways in which authors use more unusual words will go unnoticed.</p>

<h2 id="a-note-about-parts-of-speech">A Note about Parts of Speech<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#a-note-about-parts-of-speech"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">a-note-about-parts-of-speech</span></a></h2>

<p>In some languages, it may be useful to apply parts-of-speech tagging to the word tokens before counting them, so that the same word used as two different parts of speech may count as two different features. For example, in French, very common words like “la” and “le” serve both as articles (in which case they would translate into English as “the”) and as pronouns (“it”). This lesson does not use part-of-speech tagging because it is rarely useful for stylometric analysis in contemporary English and because <code class="language-plaintext highlighter-rouge">nltk</code>’s default tagger does not support other languages very well.</p>

<p>Should you need to apply part-of-speech tagging to your own data, you may be able to download taggers for other languages, to work with a third-party tool like <a href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/" target="_blank">Tree Tagger</a>, or even to train your own tagger, but these techniques are far beyond the scope of the current lesson.</p>

<h1 id="third-stylometric-test-john-burrows-delta-method-advanced">Third Stylometric Test: John Burrows’ Delta Method (Advanced)</h1>

<p>The first two stylometric methods were easy to implement. This next one, based on John Burrows’ <em>Delta</em> statistic<sup id="fnref:16" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:16" class="footnote" rel="footnote">16</a></sup>, is considerably more involved, both conceptually (the mathematics are more complicated) and computationally (more code required). It is, however, one of the most prominent stylometric methods in use today.</p>

<p>Like Kilgariff’s chi-squared, Burrows’ Delta is a measure of the “distance” between a text whose authorship we want to ascertain and some other corpus. Unlike chi-squared, however, the Delta Method is designed to compare an anonymous text (or set of texts) to many different authors’ signatures at the same time. More precisely, Delta measures how the anonymous text <em>and sets of texts written by an arbitrary number of known authors</em> all diverge from the average of all of them put together. Furthermore, the Delta Method gives equal weight to every feature that it measures, thus avoiding the problem of common words overwhelming the results, which was an issue with chi-squared tests. For all of these reasons, John Burrows’ Delta Method is usually a more effective solution to the problem of authorship.</p>

<p>Burrows’ original algorithm can be summarized as follows:</p>

<ul>
  <li>Assemble a large corpus made up of texts written by an arbitrary number of authors; let’s say that number of authors is <code class="language-plaintext highlighter-rouge">x</code>.</li>
  <li>Find the <code class="language-plaintext highlighter-rouge">n</code> most frequent words in the corpus to use as features.</li>
  <li>For each of these <code class="language-plaintext highlighter-rouge">n</code> features, calculate the share of each of the <code class="language-plaintext highlighter-rouge">x</code> authors’ subcorpora represented by this feature, as a percentage of the total number of words. As an example, the word “the” may represent 4.72% of the words in Author A’s subcorpus.</li>
  <li>Then, calculate the mean and the standard deviation of these <code class="language-plaintext highlighter-rouge">x</code> values and use them as the offical mean and standard deviation for this feature over the whole corpus. In other words, we will be using a <em>mean of means</em> instead of calculating a single value representing the share of the entire corpus represented by each word. This is because we want to avoid a larger subcorpus, like Hamilton’s in our case, over-influencing the results in its favor and defining the corpus norm in such a way that everything would be expected to look like it.</li>
  <li>For each of the <code class="language-plaintext highlighter-rouge">n</code> features and <code class="language-plaintext highlighter-rouge">x</code> subcorpora, calculate a <a href="https://en.wikipedia.org/wiki/Standard_score" target="_blank"><code class="language-plaintext highlighter-rouge">z-score</code></a> describing how far away from the corpus norm the usage of this particular feature in this particular subcorpus happens to be. To do this, subtract the “mean of means” for the feature from the feature’s frequency in the subcorpus and divide the result by the feature’s standard deviation. Figure 7 shows the z-score equation for feature ‘i’, where C(i) represents the observed frequency, the greek letter mu represents the mean of means, and the greek letter sigma, the standard deviation.</li>
</ul>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-7.jpg" alt="Figure 7: Equation for the z-score statistic.">
<figcaption>
    <p>Figure 7: Equation for the z-score statistic.</p>

</figcaption>
</figure>

<ul>
  <li>Then, calculate the same <code class="language-plaintext highlighter-rouge">z-scores</code> for each feature in the text for which we want to determine authorship.</li>
  <li>Finally, calculate a <em>delta score</em> comparing the anonymous paper with each candidate’s subcorpus. To do this, take the <em>average of the absolute values of the differences between the <code class="language-plaintext highlighter-rouge">z-scores</code> for each feature between the anonymous paper and the candidate’s subcorpus</em>. (Read that twice!) This gives equal weight to each feature, no matter how often the words occur in the texts; otherwise, the top 3 or 4 features would overwhelm everything else. Figure 8 shows the equation for Delta, where Z(c,i) is the z-score for feature ‘i’ in candidate ‘c’, and Z(t,i) is the z-score for feature ‘i’ in the test case.</li>
</ul>

<figure>
    <img src="./Introduction to stylometry with Python _ Programming Historian_files/stylometry-python-8.jpg" alt="Figure 8: Equation for John Burrows&#39; Delta statistic.">
<figcaption>
    <p>Figure 8: Equation for John Burrows’ Delta statistic.</p>

</figcaption>
</figure>

<ul>
  <li>The “winning” candidate is the author for whom the delta score between the author’s subcorpus and the test case is the lowest.</li>
</ul>

<p>Stefan Evert <em>et al</em>.<sup id="fnref:17" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:17" class="footnote" rel="footnote">17</a></sup> provide an in-depth discussion of the method’s variants, refinements and intricacies, but we will stick to the essentials for the purposes of this lesson. A different explanation of Delta, written in Spanish, and an application to a corpus of Spanish novels can also be found in a recent paper by José Calvo Tello.<sup id="fnref:18" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:18" class="footnote" rel="footnote">18</a></sup></p>

<h2 id="our-test-case">Our Test Case<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#our-test-case"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">our-test-case</span></a></h2>

<p>As our test case, we will use <em>Federalist 64</em>. Alexander Hamtilton claimed to be the author of this paper in his letter; however, a draft of <em>Federalist 64</em> was later found in John Jay’s personal papers and everyone concluded that Jay was in fact the author. No foul play is suspected, by the way: in the same letter, Hamilton attributed to Jay the authorship of another paper with a similar number that Hamilton himself had clearly written. Perhaps Hamilton was distracted by his pending duel and simply misremembered.</p>

<p>Since John Burrows’ Delta Method works with an arbitrary number of candidate authors (Burrows’ original paper uses about 25), we will compare <em>Federalist 64</em>’s stylistic signature with those of five corpora: Hamilton’s papers, Madison’s papers, Jay’s other papers, the papers co-written by Madison and Hamilton, and the papers disputed between Hamilton and Madison. We expect the Delta Method to tell us that Jay is the most likely author; any other result would call into question either the method, or the historiography, or both.</p>

<h2 id="feature-selection">Feature Selection<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#feature-selection"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">feature-selection</span></a></h2>

<p>Let’s combine all of the subcorpora into a single corpus for Delta to calculate a “standard” to work with. Then, let’s select a number of words to use as features. Remember that we used 500 words to calculate Kilgariff’s chi-squared; this time, we will use a smaller set of 30 words, most if not all of them function words and common verbs, as our features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Who are we dealing with this time?
</span><span class="n">authors</span> <span class="o">=</span> <span class="p">(</span><span class="s">"Hamilton"</span><span class="p">,</span> <span class="s">"Madison"</span><span class="p">,</span> <span class="s">"Jay"</span><span class="p">,</span> <span class="s">"Disputed"</span><span class="p">,</span> <span class="s">"Shared"</span><span class="p">)</span>

<span class="c1"># Convert papers to lowercase to count all tokens of the same word together
# regardless of case
</span><span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span><span class="n">tok</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]])</span>

<span class="c1"># Combine every paper except our test case into a single corpus
</span><span class="n">whole_corpus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">whole_corpus</span> <span class="o">+=</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">]</span>

<span class="c1"># Get a frequency distribution
</span><span class="n">whole_corpus_freq_dist</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="p">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="n">whole_corpus</span><span class="p">).</span><span class="n">most_common</span><span class="p">(</span><span class="mi">30</span><span class="p">))</span>
<span class="n">whole_corpus_freq_dist</span><span class="p">[</span> <span class="p">:</span><span class="mi">10</span> <span class="p">]</span>
</code></pre></div></div>

<p>A sample of the most frequent words with their frequency occurence looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('the', 17846),
('of', 11796),
('to', 7012),
('and', 5016),
('in', 4408),
('a', 3967),
('be', 3370),
('that', 2747),
('it', 2520),
('is', 2178)]

</code></pre></div></div>

<h2 id="calculating-features-for-each-subcorpus">Calculating features for each subcorpus<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-features-for-each-subcorpus"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">calculating-features-for-each-subcorpus</span></a></h2>

<p>Let’s look at the frequencies of each feature in each candidate’s subcorpus, as a proportion of the total number of tokens in the subcorpus. We’ll calculate these values and store them in a dictionary of dictionaries, a convenient way of building a <a href="https://en.wikipedia.org/wiki/Array_data_structure#Two-dimensional_arrays" target="_blank">two-dimensional array</a> in Python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The main data structure
</span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span><span class="n">freq</span> <span class="ow">in</span> <span class="n">whole_corpus_freq_dist</span><span class="p">]</span>
<span class="n">feature_freqs</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="c1"># A dictionary for each candidate's features
</span>    <span class="n">feature_freqs</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># A helper value containing the number of tokens in the author's subcorpus
</span>    <span class="n">overall</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">])</span>

    <span class="c1"># Calculate each feature's presence in the subcorpus
</span>    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
        <span class="n">presence</span> <span class="o">=</span> <span class="n">federalist_by_author_tokens</span><span class="p">[</span><span class="n">author</span><span class="p">].</span><span class="n">count</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="n">feature_freqs</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">presence</span> <span class="o">/</span> <span class="n">overall</span>
</code></pre></div></div>

<h2 id="calculating-feature-averages-and-standard-deviations">Calculating feature averages and standard deviations<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-feature-averages-and-standard-deviations"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">calculating-feature-averages-and-standard-deviations</span></a></h2>

<p>Given the feature frequencies for all four subcorpora that we have just computed, we can find a “mean of means” and a standard deviation for each feature. We’ll store these values in another “dictionary of dictionaries”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># The data structure into which we will be storing the "corpus standard" statistics
</span><span class="n">corpus_features</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># For each feature...
</span><span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="c1"># Create a sub-dictionary that will contain the feature's mean
</span>    <span class="c1"># and standard deviation
</span>    <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Calculate the mean of the frequencies expressed in the subcorpora
</span>    <span class="n">feature_average</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
        <span class="n">feature_average</span> <span class="o">+=</span> <span class="n">feature_freqs</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span>
    <span class="n">feature_average</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">authors</span><span class="p">)</span>
    <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"Mean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_average</span>

    <span class="c1"># Calculate the standard deviation using the basic formula for a sample
</span>    <span class="n">feature_stdev</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">feature_freqs</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span> <span class="o">-</span> <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"Mean"</span><span class="p">]</span>
        <span class="n">feature_stdev</span> <span class="o">+=</span> <span class="n">diff</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">feature_stdev</span> <span class="o">/=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">authors</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">feature_stdev</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">feature_stdev</span><span class="p">)</span>
    <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"StdDev"</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_stdev</span>
</code></pre></div></div>

<h2 id="calculating-z-scores">Calculating z-scores<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-z-scores"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">calculating-z-scores</span></a></h2>

<p>Next, we transform the observed feature frequencies in the five candidates’ subcorpora into <code class="language-plaintext highlighter-rouge">z-scores</code> describing how far away from the “corpus norm” these observations are. Nothing fancy here: we merely apply the definition of the <code class="language-plaintext highlighter-rouge">z-score</code> to each feature and store the results into yet another two-dimensional array.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_zscores</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">feature_zscores</span><span class="p">[</span><span class="n">author</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>

        <span class="c1"># Z-score definition = (value - mean) / stddev
</span>        <span class="c1"># We use intermediate variables to make the code easier to read
</span>        <span class="n">feature_val</span> <span class="o">=</span> <span class="n">feature_freqs</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span>
        <span class="n">feature_mean</span> <span class="o">=</span> <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"Mean"</span><span class="p">]</span>
        <span class="n">feature_stdev</span> <span class="o">=</span> <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"StdDev"</span><span class="p">]</span>
        <span class="n">feature_zscores</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">feature_val</span><span class="o">-</span><span class="n">feature_mean</span><span class="p">)</span> <span class="o">/</span>
                                            <span class="n">feature_stdev</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="calculating-features-and-z-scores-for-our-test-case">Calculating features and z-scores for our test case<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-features-and-z-scores-for-our-test-case"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">calculating-features-and-z-scores-for-our-test-case</span></a></h2>

<p>Next, we need to compare <em>Federalist 64</em> with the corpus. The following code snippet, which essentially recapitulates everything we have done so far, counts the frequencies of each of our 30 features in <em>Federalist 64</em> and calculates <code class="language-plaintext highlighter-rouge">z-scores</code> accordingly:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Tokenize the test case
</span><span class="n">testcase_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">federalist_by_author</span><span class="p">[</span><span class="s">"TestCase"</span><span class="p">])</span>

<span class="c1"># Filter out punctuation and lowercase the tokens
</span><span class="n">testcase_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">testcase_tokens</span>
                   <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">token</span><span class="p">)]</span>

<span class="c1"># Calculate the test case's features
</span><span class="n">overall</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testcase_tokens</span><span class="p">)</span>
<span class="n">testcase_freqs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">presence</span> <span class="o">=</span> <span class="n">testcase_tokens</span><span class="p">.</span><span class="n">count</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
    <span class="n">testcase_freqs</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">presence</span> <span class="o">/</span> <span class="n">overall</span>

<span class="c1"># Calculate the test case's feature z-scores
</span><span class="n">testcase_zscores</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">feature_val</span> <span class="o">=</span> <span class="n">testcase_freqs</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>
    <span class="n">feature_mean</span> <span class="o">=</span> <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"Mean"</span><span class="p">]</span>
    <span class="n">feature_stdev</span> <span class="o">=</span> <span class="n">corpus_features</span><span class="p">[</span><span class="n">feature</span><span class="p">][</span><span class="s">"StdDev"</span><span class="p">]</span>
    <span class="n">testcase_zscores</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">feature_val</span> <span class="o">-</span> <span class="n">feature_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">feature_stdev</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Test case z-score for feature"</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="s">"is"</span><span class="p">,</span> <span class="n">testcase_zscores</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>
</code></pre></div></div>

<p>The results of some features z-scores for <em>Federalist 64</em> should look like this (a sample):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test case z-score for feature the is -0.7692828380408238
Test case z-score for feature of is -1.8167784558461264
Test case z-score for feature to is 1.032705844508835
Test case z-score for feature and is 1.0268752924746058
Test case z-score for feature in is 0.6085448501260903
Test case z-score for feature a is -0.9341289591084886
Test case z-score for feature be is 1.0279650702511498

</code></pre></div></div>

<h2 id="calculating-delta">Calculating Delta<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-delta"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">calculating-delta</span></a></h2>

<p>And finally, we use the formula for Delta defined by Burrows to extract a single score comparing <em>Federalist 64</em> with each of the five “candidate authors”. Reminder: the smaller the Delta score, the more similar <em>Federalist 64</em>’s stylometric signature is to the candidate’s.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="n">authors</span><span class="p">:</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">+=</span> <span class="n">math</span><span class="p">.</span><span class="n">fabs</span><span class="p">((</span><span class="n">testcase_zscores</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">-</span>
                            <span class="n">feature_zscores</span><span class="p">[</span><span class="n">author</span><span class="p">][</span><span class="n">feature</span><span class="p">]))</span>
    <span class="n">delta</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span> <span class="s">"Delta score for candidate"</span><span class="p">,</span> <span class="n">author</span><span class="p">,</span> <span class="s">"is"</span><span class="p">,</span> <span class="n">delta</span> <span class="p">)</span>
</code></pre></div></div>

<p>The results: Delta scores suggest that John Jay indeed wrote <em>Federalist 64</em>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Delta score for candidate Hamilton is 1.768470453004334
Delta score for candidate Madison is 1.6089724119682816
Delta score for candidate Jay is 1.5345768956569326
Delta score for candidate Disputed is 1.5371768107570636
Delta score for candidate Shared is 1.846113566619675

</code></pre></div></div>

<p>As expected, Delta identifies John Jay as <em>Federalist 64</em>’s most likely author. It is interesting to note that, according to Delta, <em>Federalist 64</em> is more similar to the disputed papers than to those known to have been written by Hamilton or by Madison; why that might be, however, is a question for another day.</p>

<h1 id="further-reading-and-resources">Further Reading and Resources</h1>

<h2 id="interesting-case-studies">Interesting case studies<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#interesting-case-studies"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">interesting-case-studies</span></a></h2>

<p>Stylometry and/or authorship attribution have been used in many contexts, employing many techniques. Here are but a few interesting case studies:</p>

<ul>
  <li>Javier de la Rosa and Juan Luis Suárez look for the author or a famous 16th-century Spanish novel from among a considerable list of candidates.<sup id="fnref:19" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:19" class="footnote" rel="footnote">19</a></sup></li>
  <li>Maria Slautina and Mikhail Marusenko use pattern recognition on a set of syntactic, grammatical and lexical features, from simple word counts (with part-of-speech tagging) to various types of phrases, in order to establish stylistic similarity between medieval texts.<sup id="fnref:20" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:20" class="footnote" rel="footnote">20</a></sup></li>
  <li>Ellen Jordan, Hugh Craig and Alexis Antonia look at the case of 19th-century British periodicals, in which articles were usually unsigned, to determine the author of four reviews of works by or about the Brontë sisters.<sup id="fnref:21" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:21" class="footnote" rel="footnote">21</a></sup> This case study applies an early version of another method developed by John Burrows, the Zeta method, which focuses on an author’s favorite words instead of common function words.<sup id="fnref:22" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:22" class="footnote" rel="footnote">22</a></sup></li>
  <li>Valérie Beaudoin and François Yvon analyse 58 plays in verse by French playwrights Corneille, Racine and Molière, finding that the first two were far more consistent in the way they structured their writing than the latter.<sup id="fnref:23" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:23" class="footnote" rel="footnote">23</a></sup></li>
  <li>Marcelo Luiz Brocardo, Issa Traore, Sherif Saad and Isaac Woungang apply <a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank">supervised learning</a> and <a href="https://en.wikipedia.org/wiki/N-gram" target="_blank">n-gram models</a> to determine the authorship of short messages with large numbers of potential authors, like emails and tweets.<sup id="fnref:24" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:24" class="footnote" rel="footnote">24</a></sup></li>
  <li>Moshe Koppel and Winter Yaron propose the “impostor method”, which attempts to determine whether two texts have been written by the same author by inserting them into a set of texts written by false candidates.<sup id="fnref:25" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:25" class="footnote" rel="footnote">25</a></sup> Justin Anthony Stover <em>et al.</em> have recently applied the technique to determine the authorship of a newly discovered 2nd-century manuscript.<sup id="fnref:26" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:26" class="footnote" rel="footnote">26</a></sup></li>
  <li>Finally, a team led by David I. Holmes studies the peculiar case of documents written either by a Civil War soldier or by his widow who may intentionally have copied his writing style.<sup id="fnref:27" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:27" class="footnote" rel="footnote">27</a></sup></li>
</ul>

<h2 id="additional-references-on-authorship-and-stylometry">Additional references on authorship and stylometry<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#additional-references-on-authorship-and-stylometry"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">additional-references-on-authorship-and-stylometry</span></a></h2>

<p>The most exhaustive reference in all matters related to authorship attribution, including the history of the field, its mathematical and linguistic underpinnings, and its various methods, was written by Patrick Juola in 2007.<sup id="fnref:28" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:28" class="footnote" rel="footnote">28</a></sup> Chapter 7, in particular, shows how authorship attribution can serve as a marker for various group identities (gender, nationality, dialect, etc.), for change in language over time, and even for personality and mental health.</p>

<p>A shorter survey can be found in Moshe Koppel <em>et al.</em>, who discuss cases in which there is a single candidate author whose authorship must be confirmed, large numbers of candidates for which only small writing samples are available to train a machine learning algorithm, or no known candidate at all.<sup id="fnref:29" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:29" class="footnote" rel="footnote">29</a></sup></p>

<p>The Stamatatos paper cited earlier<sup id="fnref:2:2" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:2" class="footnote" rel="footnote">2</a></sup> also contains a quality survey of the field.</p>

<h2 id="varia">Varia<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#varia"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">varia</span></a></h2>

<p>Programming historians who wish to explore stylometry further may want to download the Stylo package<sup id="fnref:30" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:30" class="footnote" rel="footnote">30</a></sup>, which has become a <em>de facto</em> standard. Among other things, Stylo provides an implementation of the Delta method, feature extraction functionality, and convenient graphical user interfaces for both data manipulation and the production of visually appealing results. Note that Stylo is written in <a href="https://www.r-project.org/" target="_blank">R</a>, which means that you will need R installed on your computer to run it, but between the graphical user interface and the tutorials, little or no prior knowledge of R programming should be necessary.</p>

<p>Readers fluent in French who are interested in exploring the <a href="https://en.wikipedia.org/wiki/Epistemology" target="_blank">epistemological</a> implications of the interactions between quantitative and qualitative methods in the analysis of writing style should read Clémence Jacquot.<sup id="fnref:31" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:31" class="footnote" rel="footnote">31</a></sup></p>

<p>Somewhat surprisingly, data obtained through <a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank">optical character recognition</a> (OCR) have been shown to be adequate for authorship attribution purposes, even when the data suffer from high OCR error rates.<sup id="fnref:32" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:32" class="footnote" rel="footnote">32</a></sup></p>

<p>Readers interested in further discussion of the history of the <em>Federalist Papers</em> and of the various theories advanced regarding their authorship may want to start by reading papers by Irving Brant<sup id="fnref:33" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:33" class="footnote" rel="footnote">33</a></sup> and by Paul Ford and Edward Bourne.<sup id="fnref:34" role="doc-noteref"><a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fn:34" class="footnote" rel="footnote">34</a></sup> The topic, however, is almost boundless.</p>

<p>Finally, there is a <a href="https://www.zotero.org/groups/643516/stylometry_bibliography/items" target="_blank">Zotero group</a> dedicated to stylometry, where you can find many more references to methods and studies.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>Thanks to Stéfan Sinclair and Andrew Piper, in whose seminars at McGill University this project began. Also thanks to my thesis advisor, Susan Dalton, whose mentorship in always invaluable.</p>

<h1 id="endnotes">Endnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>See, for example, Justin Rice, <a href="https://www.litcharts.com/analitics/hemingway" target="_blank">“What Makes Hemingway Hemingway? A statistical analysis of the data behind Hemingway’s style”</a>&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Efstathios Stamatatos, “A Survey of Modern Authorship Attribution Method,” <em>Journal of the American Society for Information Science and Technology</em>, vol. 60, no. 3 (December 2008), p. 538–56, citation on p. 540, https://doi.org/10.1002/asi.21001.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:2" class="reversefootnote" role="doc-backlink">↩</a>&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:2:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a>&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:2:2" class="reversefootnote" role="doc-backlink">↩<sup>3</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Jan Rybicki, “Vive La Différence: Tracing the (Authorial) Gender Signal by Multivariate Analysis of Word Frequencies,” <em>Digital Scholarship in the Humanities</em>, vol. 31, no. 4 (December 2016), pp. 746–61, https://doi.org/10.1093/llc/fqv023. Sean G. Weidman and James O’Sullivan, “The Limits of Distinctive Words: Re-Evaluating Literature’s Gender Marker Debate,” <em>Digital Scholarship in the Humanities</em>, 2017, https://doi.org/10.1093/llc/fqx017.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Ted Underwood, David Bamman, and Sabrina Lee, “The Transformation of Gender in English-Language Fiction”, <em>Cultural Analytics</em>, Feb. 13, 2018,&nbsp;DOI:&nbsp;10.7910/DVN/TEGMGI.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Sven Meyer zu Eissen and Benno Stein, “Intrinsic Plagiarism Detection,” in <em>ECIR 2006</em>, edited by Mounia Lalmas, Andy MacFarlane, Stefan Rüger, Anastasios Tombros, Theodora Tsikrika, and Alexei Yavlinsky, Berlin, Heidelberg: Springer, 2006, pp. 565–69, https://doi.org/10.1007/11735106_66.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Cynthia Whissell, “Traditional and Emotional Stylometric Analysis of the Songs of Beatles Paul McCartney and John Lennon,” <em>Computers and the Humanities</em>, vol. 30, no. 3 (1996), pp. 257–65.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Douglass Adair, “The Authorship of the Disputed Federalist Papers”, <em>The William and Mary Quarterly</em>, vol. 1, no. 2 (April 1944), pp. 97-122.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:7" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>David I. Holmes and Richard S. Forsyth, “The Federalist Revisited: New Directions in Authorship Attribution”, <em>Literary and Linguisting Computing</em>, vol. 10, no. 2 (1995), pp. 111-127.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:8" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Frederick Mosteller, “A Statistical Study of the Writing Styles of the Authors of the Federalist Papers”, <em>Proceedings of the American Philosophical Society</em>, vol. 131, no. 2 (1987), pp. 132‑40.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:9" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Frederick Mosteller and David Lee Wallace, <em>Inference and Disputed Authorship: The Federalist</em>, Addison-Wesley Series in Behavioral Science : Quantitative Methods (Reading, Mass.: Addison-Wesley PublCo, 1964).&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:10" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>See for example Glenn Fung, “The disputed Federalist papers: SVM feature selection via concave minimization”, <em>TAPIA ‘03: Proceedings of the 2003 conference on Diversity in Computing</em>, pp. 42-46; and Robert A. Bosch and Jason A. Smith, “Separating Hyperplanes and the Authorship of the Disputed Federalist Papers,” <em>The American Mathematical Monthly</em>, vol. 105, no. 7 (1998), pp. 601–8, https://doi.org/10.2307/2589242.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:11" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>Jeff Collins, David Kaufer, Pantelis Vlachos, Brian Butler and Suguru Ishizaki, “Detecting Collaborations in Text: Comparing the Authors’ Rhetorical Language Choices in The Federalist Papers”, <em>Computers and the Humanities</em>, vol. 38 (2004), pp. 15-36.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:12" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>Mosteller, “A Statistical Study…”, pp. 132-133.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:13" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>T. C. Mendenhall, “The Characteristic Curves of Composition”, <em>Science</em>, vol. 9, no. 214 (Mar. 11, 1887), pp. 237-249.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:14" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p>Adam Kilgarriff, “Comparing Corpora”, <em>International Journal of Corpus Linguistics</em>, vol. 6, no. 1 (2001), pp. 97-133.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:15" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p>John Burrows, “‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship”, <em>Literary and Linguistic Computing</em>, vol. 17, no. 3 (2002), pp. 267-287.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:16" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p>Stefan Evert et al., “Understanding and explaining Delta measures for authorship attribution”, <em>Digital Scholarship in the Humanities</em>, vol. 32, no. suppl_2 (2017), pp.  ii4-ii16.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:17" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:18" role="doc-endnote">
      <p>José Calvo Tello, “Entendiendo Delta desde las Humanidades,” <em>Caracteres</em>, May 27 2016, http://revistacaracteres.net/revista/vol5n1mayo2016/entendiendo-delta/.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:18" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p>Javier de la Rosa and Juan Luis Suárez, “The Life of Lazarillo de Tormes and of His Machine Learning Adversities,” <em>Lemir</em>, vol. 20 (2016), pp. 373-438.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:19" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:20" role="doc-endnote">
      <p>Maria Slautina and Mikhaïl Marusenko, “L’émergence du style, The emergence of style,” <em>Les Cahiers du numérique</em>, vol. 10, no. 4 (November 2014), pp. 179–215, https://doi.org/10.3166/LCN.10.4.179-215.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:20" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:21" role="doc-endnote">
      <p>Ellen Jordan, Hugh Craig, and Alexis Antonia, “The Brontë Sisters and the ‘Christian Remembrancer’: A Pilot Study in the Use of the ‘Burrows Method’ to Identify the Authorship of Unsigned Articles in the Nineteenth-Century Periodical Press,” <em>Victorian Periodicals Review</em>, vol. 39, no. 1 (2006), pp. 21–45.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:21" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:22" role="doc-endnote">
      <p>John Burrows, “All the Way Through: Testing for Authorship in Different Frequency Strata,” <em>Literary and Linguistic Computing</em>, vol. 22, no. 1 (April 2007), pp. 27–47, https://doi.org/10.1093/llc/fqi067.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:22" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:23" role="doc-endnote">
      <p>Valérie Beaudoin and François Yvon, “Contribution de La Métrique à La Stylométrie,” <em>JADT 2004: 7e Journées internationales d’Analyse statistique des Données Textuelles</em>, vol. 1, Louvain La Neuve, Presses Universitaires de Louvain, 2004, pp. 107–18.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:23" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:24" role="doc-endnote">
      <p>Marcelo Luiz Brocardo, Issa Traore, Sherif Saad and Isaac Woungang, “Authorship Verification for Short Messages Using Stylometry,” <em>2013 International Conference on Computer, Information and Telecommunication Systems (CITS)</em>, 2013, https://doi.org/10.1109/CITS.2013.6705711.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:24" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:25" role="doc-endnote">
      <p>Moshe Koppel and Winter Yaron, “Determining If Two Documents Are Written by the Same Author,” <em>Journal of the Association for Information Science and Technology</em>, vol. 65, no. 1 (October 2013), pp. 178–87, https://doi.org/10.1002/asi.22954.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:25" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:26" role="doc-endnote">
      <p>Justin Anthony Stover et al., “Computational authorship verification method attributes a new work to a major 2nd century African author”, <em>Journal of the Association for Information Science and Technology</em>, vol. 67, no. 1 (2016), pp. 239–242.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:26" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:27" role="doc-endnote">
      <p>David I. Holmes, Lesley J. Gordon, and Christine Wilson, “A widow and her soldier: Stylometry and the American Civil War”, <em>Literary and Linguistic Computing</em>, vol. 16, no 4 (2001), pp. 403–420.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:27" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:28" role="doc-endnote">
      <p>Patrick  Juola, “Authorship Attribution,” <em>Foundations and Trends in Information Retrieval</em>, vol. 1, no. 3 (2007), pp. 233–334, https://doi.org/10.1561/1500000005.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:28" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:29" role="doc-endnote">
      <p>Moshe Koppel, Jonathan Schler, and Shlomo Argamon, “Computational Methods in Authorship Attribution,” <em>Journal of the Association for Information Science and Technology</em>. vol. 60, no. 1 (January 2009), pp. 9–26, https://doi.org/10.1002/asi.v60:1.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:29" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:30" role="doc-endnote">
      <p>Maciej Eder, Jan Rybicki, and Mike Kestemont, “Stylometry with R: A Package for Computational Text Analysis,” <em>The R Journal</em>, vol. 8, no. 1 (2016), pp. 107–21.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:30" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:31" role="doc-endnote">
      <p>Clémence Jacquot, “Rêve d’une épiphanie du style: visibilité et saillance en stylistique et en stylométrie,” <em>Revue d’Histoire Littéraire de la France</em> , vol. 116, no. 3 (2016),  pp. 619–39.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:31" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:32" role="doc-endnote">
      <p>Patrick Juola, John Noecker Jr, and Michael Ryan, “Authorship Attribution and Optical Character Recognition Errors”, <em>TAL</em>, vol. 53, no. 3 (2012), pp. 101–127.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:32" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:33" role="doc-endnote">
      <p>Irving Brant, “Settling the Authorship of the Federalist”, <em>The American Historical Review</em>, vol. 67, no. 1 (October 1961), pp. 71-75.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:33" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:34" role="doc-endnote">
      <p>Paul Leicester Ford and Edward Gaylord Bourne, “The Authorship of the Federalist”, <em>The American Historical Review</em>, vol. 2, no. 4 (July 1897), pp. 675-687.&nbsp;<a href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#fnref:34" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div>

 



  



<div class="author-info">
    <h2 class="author-name">About the author</h2>
</div>
<div class="author-description">
  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
         <p>François Dominic Laramée is a postdoctoral fellow in digital history of medicine at the University of Ottawa, in Canada.
He holds master's degrees in computer science and U.S. history and is a former video game designer, TV personality and screenwriter.
 <a href="https://orcid.org/0000-0001-5542-3754" target="_blank"><img src="./Introduction to stylometry with Python _ Programming Historian_files/ORCIDiD_iconvector.svg" alt="ORCID id icon" width="16px" style="max-width:16px;display:inline;"></a></p><p>
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

</p></div>


  <div class="citation-info">
    
    
    

    <h2 class="suggested-citation-header">Suggested Citation</h2>
    <div class="suggested-citation-text">
      <p class="suggested-citation-text">



François Dominic Laramée,
        "Introduction to stylometry with Python,"
        <em>The Programming Historian</em> 7 (2018),
        https://doi.org/10.46430/phen0078.</p>
    </div>
  </div>

  
  
<div class="alert alert-success hide-screen"><h2 id="donate-today">Donate today!<a class="header-link" href="https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#donate-today"><i class="fa fa-link" style="font-size: 0.8em"></i><span class="visually-hidden">donate-today</span></a></h2>
<p>Great Open Access tutorials cost money to produce. Join the growing number of people <a href="https://www.patreon.com/theprogramminghistorian" target="_blank">supporting <em>The Programming Historian</em></a> so we can continue to share knowledge free of charge.</p>
</div>

  

</div>


<script>
  (function () {
    var githubAPI = "https://api.github.com/repos/programminghistorian/jekyll/commits";
    $.getJSON(githubAPI, {
      path: "en/lessons/introduction-to-stylometry-with-python.md"
    })
      .done(function (data) {
        var date = new Date(data[0].commit.author.date);
        var formatted_date = new Intl.DateTimeFormat('sv').format(date)
        $("#modified-date").text(formatted_date);
      });
  })();
</script>

      

<footer role="contentinfo" style="background-color: #444444">


<div class="d-flex flex-wrap justify-content-center footer-head">
  <p><em>The Programming Historian</em> (ISSN: 2397-2068) is released under a <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC-BY</a> license.</p>
<p>This project is administered by ProgHist Limited, Company Number <a href="https://beta.companieshouse.gov.uk/company/12192946" target="_blank">12192946</a>.</p>

</div>

<div class="d-flex flex-wrap justify-content-around">
  <div class="mx-4">
     
    <strong>
      <p>
        <a href="https://programminghistorian.org/">ISSN 2397-2068 (English)</a>
      </p>
      </strong>  
      <p>
        <a href="https://programminghistorian.org/es">ISSN 2517-5769 (Spanish)</a>
      </p>
        
      <p>
        <a href="https://programminghistorian.org/fr">ISSN 2631-9462 (French)</a>
      </p>
        
      <p>
        <a href="https://programminghistorian.org/pt">ISSN 2753-9296 (Portuguese)</a>
      </p>
       
  </div>
  <div class="mx-4">
    <i class="fab fa-github" aria-hidden="true"></i>
    <a href="https://github.com/programminghistorian/jekyll" target="_blank">Hosted on GitHub</a>
  </div>

  <div class="mx-4">
  <i class="fa fa-calendar" aria-hidden="true"></i>
    <a href="https://github.com/programminghistorian/jekyll/commits/gh-pages" target="_blank">Site last updated 04 July 2021</a>
  </div>
 
 <div class="mx-4">
  <i class="fas fa-rss" aria-hidden="true"></i>
  <a href="https://programminghistorian.org/feed.xml">RSS feed subscriptions</a>
  </div>
 
  <div class="mx-4">
  <i class="fa fa-history" aria-hidden="true"></i>
  <a href="https://github.com/programminghistorian/jekyll/commits/gh-pages/en/lessons/introduction-to-stylometry-with-python.md" target="_blank">See page history</a>
  </div>

  <div class="mx-4">
  <i class="fa fa-bolt" aria-hidden="true"></i>
  <a href="https://programminghistorian.org/en/feedback">Make a suggestion</a>
  </div>

  <div class="mx-4">
  <i class="fa fa-chain-broken" aria-hidden="true"></i>
  <a href="https://programminghistorian.org/en/lesson-retirement-policy">Lesson retirement policy</a>
  </div>
 
  <div class="mx-4">
  <i class="fa fa-globe" aria-hidden="true"></i>
  <a href="https://programminghistorian.org/translation-concordance">Translation concordance</a>
  </div>
 
  </div>

</footer>



<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2752866-8']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


    </main>
  


</body></html>